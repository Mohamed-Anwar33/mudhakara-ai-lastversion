import type { SupabaseClient } from '@supabase/supabase-js';
import { processPdfJob } from './pdf-processor.js';
import { embedLessonSections } from './embeddings.js';
import { generateLessonAnalysis } from './analysis.js';

/**
 * Book-to-Lessons Auto Segmenter
 *
 * Takes a FULL textbook PDF â†’ uses AI to detect lesson/chapter
 * boundaries â†’ creates individual lesson records â†’ feeds each into the
 * existing AI pipeline (extract â†’ embed â†’ analyze).
 *
 * Detection Strategy (Multi-Model):
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * 1. PRIMARY: Gemini Vision (best for Arabic PDFs, handles large files)
 * 2. FALLBACK: GPT-4o Vision (if Gemini fails or quality is poor)
 * 3. Quality Validation: checks page coverage, overlaps, gaps
 * 4. Cross-reference: if both models succeed, pick the best result
 * 5. Last resort: equal-page splitting (~10 pages per lesson)
 *
 * For each detected lesson we:
 *   a) Create a `lessons` row in the DB
 *   b) Extract only that lesson's pages
 *   c) Generate embeddings
 *   d) Run full analysis (summary, focus, quizzes)
 */

// â”€â”€â”€ Types â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export interface DetectedLesson {
    title: string;
    startPage: number;
    endPage: number;
    description?: string;
}

export interface SegmentationResult {
    totalPages: number;
    lessonsDetected: number;
    lessons: Array<{
        id: string;
        title: string;
        startPage: number;
        endPage: number;
        status: 'created' | 'processed' | 'analyzed' | 'failed';
        error?: string;
    }>;
    method: 'gemini' | 'gpt4o' | 'cross-validated' | 'fallback-split';
}

// â”€â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const FALLBACK_PAGES_PER_LESSON = 10;
const MAX_LESSONS_DETECTED = 50;
const MIN_LESSONS_DETECTED = 1;
const MIN_QUALITY_SCORE = 0.5;  // 50% page coverage threshold

// â”€â”€â”€ Shared Prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function buildDetectionPrompt(pageCount: number): string {
    return `Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØªØ¨ Ø§Ù„Ù…Ø¯Ø±Ø³ÙŠØ©. Ø³Ø£Ø¹Ø·ÙŠÙƒ Ù…Ù„Ù PDF Ù„ÙƒØªØ§Ø¨ Ù…Ø¯Ø±Ø³ÙŠ ÙƒØ§Ù…Ù„ (${pageCount} ØµÙØ­Ø©).

Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: Ø­Ø¯Ø¯ ÙƒÙ„ Ø¯Ø±Ø³ Ø£Ùˆ ÙØµÙ„ Ø£Ùˆ ÙˆØ­Ø¯Ø© ÙÙŠ Ø§Ù„ÙƒØªØ§Ø¨.

Ù„ÙƒÙ„ Ø¯Ø±Ø³ Ø£Ø¹Ø·Ù†ÙŠ:
1. **title**: Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø¯Ø±Ø³ Ø¨Ø§Ù„Ø¶Ø¨Ø· ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙƒØªÙˆØ¨ ÙÙŠ Ø§Ù„ÙƒØªØ§Ø¨
2. **startPage**: Ø±Ù‚Ù… Ø§Ù„ØµÙØ­Ø© Ø§Ù„ØªÙŠ ÙŠØ¨Ø¯Ø£ ÙÙŠÙ‡Ø§ Ø§Ù„Ø¯Ø±Ø³ (1-indexed)
3. **endPage**: Ø±Ù‚Ù… Ø¢Ø®Ø± ØµÙØ­Ø© ÙÙŠ Ø§Ù„Ø¯Ø±Ø³ (1-indexed)
4. **description**: ÙˆØµÙ Ù…Ø®ØªØµØ± Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¯Ø±Ø³ (Ø¬Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø©)

âš ï¸ Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ù‡Ù…Ø©:
- Ø­Ù„Ù„ Ø§Ù„ÙƒØªØ§Ø¨ ÙƒØ§Ù…Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù„Ù„Ø£Ø®ÙŠØ±Ø©
- Ù„Ø§ ØªØªØ®Ø·Ù‰ Ø£ÙŠ Ø¯Ø±Ø³ Ø£Ùˆ ÙØµÙ„
- startPage Ù„Ù„Ø¯Ø±Ø³ Ø§Ù„ØªØ§Ù„ÙŠ ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø¨Ø¹Ø¯ endPage Ù„Ù„Ø¯Ø±Ø³ Ø§Ù„Ø³Ø§Ø¨Ù‚
- Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø¨Ø§Ù„Ø¶Ø¨Ø· ÙƒÙ…Ø§ ÙÙŠ Ø§Ù„ÙƒØªØ§Ø¨ (Ù„Ø§ ØªØ¹Ø¯Ù‘Ù„)
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ù…Ù‚Ø¯Ù…Ø© Ø£Ùˆ ÙÙ‡Ø±Ø³ØŒ Ù„Ø§ ØªØ¹ØªØ¨Ø±Ù‡Ø§ Ø¯Ø±Ø³Ø§Ù‹
- ØªØ£ÙƒØ¯ Ø£Ù† Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ØµÙØ­Ø§Øª ØµØ­ÙŠØ­Ø© ÙˆÙ…ØªØ³Ù„Ø³Ù„Ø©
- Ø£Ø¹Ø¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙƒÙ€ JSON array ÙÙ‚Ø·

Ø§Ù„ØµÙŠØºØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© (JSON ÙÙ‚Ø·):
[
  { "title": "Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø¯Ø±Ø³", "startPage": 1, "endPage": 15, "description": "ÙˆØµÙ Ù…Ø®ØªØµØ±" },
  ...
]`;
}

// â”€â”€â”€ Response Parser & Sanitizer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function parseAndSanitize(content: string, pageCount: number): DetectedLesson[] {
    let lessons: DetectedLesson[];

    try {
        lessons = JSON.parse(content);
    } catch {
        const match = content.match(/```json\s*([\s\S]*?)```/);
        if (match) {
            lessons = JSON.parse(match[1]);
        } else {
            throw new Error(`Bad JSON: ${content.substring(0, 300)}`);
        }
    }

    if (!Array.isArray(lessons)) throw new Error('Response is not an array');

    lessons = lessons
        .filter(l => l.title && typeof l.startPage === 'number' && typeof l.endPage === 'number')
        .map(l => ({
            title: String(l.title).trim(),
            startPage: Math.max(1, Math.min(l.startPage, pageCount)),
            endPage: Math.max(1, Math.min(l.endPage, pageCount)),
            description: l.description ? String(l.description).trim() : undefined
        }))
        .filter(l => l.endPage >= l.startPage)
        .slice(0, MAX_LESSONS_DETECTED);

    lessons.sort((a, b) => a.startPage - b.startPage);
    return lessons;
}

// â”€â”€â”€ Quality Scorer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

interface QualityReport {
    score: number;       // 0.0 â€“ 1.0
    pageCoverage: number;
    hasOverlaps: boolean;
    hasLargeGaps: boolean;
    avgPagesPerLesson: number;
    issues: string[];
}

function assessQuality(lessons: DetectedLesson[], pageCount: number): QualityReport {
    const issues: string[] = [];
    if (lessons.length === 0) return { score: 0, pageCoverage: 0, hasOverlaps: false, hasLargeGaps: false, avgPagesPerLesson: 0, issues: ['No lessons'] };

    // 1. Page coverage: what % of the book is covered?
    const coveredPages = new Set<number>();
    for (const l of lessons) {
        for (let p = l.startPage; p <= l.endPage; p++) coveredPages.add(p);
    }
    const pageCoverage = coveredPages.size / pageCount;
    if (pageCoverage < 0.5) issues.push(`Low coverage: ${(pageCoverage * 100).toFixed(0)}%`);

    // 2. Overlaps: any two lessons share pages?
    let hasOverlaps = false;
    for (let i = 1; i < lessons.length; i++) {
        if (lessons[i].startPage <= lessons[i - 1].endPage) {
            hasOverlaps = true;
            issues.push(`Overlap: "${lessons[i - 1].title}" & "${lessons[i].title}"`);
        }
    }

    // 3. Large gaps: more than 5 pages between consecutive lessons?
    let hasLargeGaps = false;
    for (let i = 1; i < lessons.length; i++) {
        const gap = lessons[i].startPage - lessons[i - 1].endPage - 1;
        if (gap > 5) {
            hasLargeGaps = true;
            issues.push(`Gap of ${gap} pages between "${lessons[i - 1].title}" & "${lessons[i].title}"`);
        }
    }

    // 4. Average pages per lesson (sanity check)
    const totalLessonPages = lessons.reduce((s, l) => s + (l.endPage - l.startPage + 1), 0);
    const avgPagesPerLesson = totalLessonPages / lessons.length;
    if (avgPagesPerLesson < 2) issues.push(`Avg ${avgPagesPerLesson.toFixed(1)} pages/lesson â€” too small`);
    if (avgPagesPerLesson > pageCount * 0.5) issues.push(`Avg ${avgPagesPerLesson.toFixed(1)} pages/lesson â€” too large`);

    // 5. Score (0-1)
    let score = pageCoverage;
    if (hasOverlaps) score *= 0.7;
    if (hasLargeGaps) score *= 0.85;
    if (lessons.length < 2 && pageCount > 20) score *= 0.5;

    return { score, pageCoverage, hasOverlaps, hasLargeGaps, avgPagesPerLesson, issues };
}

// â”€â”€â”€ Model 1: Gemini Vision â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function detectWithGemini(buffer: Buffer, pageCount: number): Promise<DetectedLesson[]> {
    const apiKey = process.env.GEMINI_API_KEY;
    if (!apiKey) throw new Error('GEMINI_API_KEY not set');

    const base64 = buffer.toString('base64');
    const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=${apiKey}`;

    console.log(`[BookSegmenter] ğŸ” Gemini Vision: detecting lessons in ${pageCount}-page PDF...`);

    const response = await fetch(url, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
            contents: [{
                parts: [
                    { text: buildDetectionPrompt(pageCount) },
                    { inlineData: { data: base64, mimeType: 'application/pdf' } }
                ]
            }],
            generationConfig: {
                temperature: 0.1,
                maxOutputTokens: 16384,
                responseMimeType: 'application/json'
            }
        })
    });

    const data = await response.json();
    if (!response.ok) throw new Error(`Gemini: ${data.error?.message || response.status}`);

    const content = data.candidates?.[0]?.content?.parts?.[0]?.text || '';
    if (!content) throw new Error('Gemini empty response');

    const lessons = parseAndSanitize(content, pageCount);
    console.log(`[BookSegmenter] Gemini found ${lessons.length} lessons`);
    return lessons;
}

// â”€â”€â”€ Model 2: GPT-4o Vision (Fallback) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function detectWithGPT4o(buffer: Buffer, pageCount: number): Promise<DetectedLesson[]> {
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) throw new Error('OPENAI_API_KEY not set');

    // GPT-4o only accepts images, not PDFs directly.
    // We send a text description + ask it to analyze based on page count.
    // For actual PDF Vision, we use a workaround: convert first/last pages to base64 image
    // OR rely on the text prompt with context about page count.

    console.log(`[BookSegmenter] ğŸ” GPT-4o: detecting lessons in ${pageCount}-page PDF...`);

    // Strategy: Send the PDF as base64 in a data URL â€” GPT-4o-supports PDF input via URL
    const base64 = buffer.toString('base64');

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
            'Authorization': `Bearer ${apiKey}`,
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({
            model: 'gpt-4o',
            messages: [
                {
                    role: 'user',
                    content: [
                        { type: 'text', text: buildDetectionPrompt(pageCount) },
                        {
                            type: 'image_url',
                            image_url: {
                                url: `data:application/pdf;base64,${base64}`
                            }
                        }
                    ]
                }
            ],
            temperature: 0.1,
            max_tokens: 16384,
            response_format: { type: 'json_object' }
        })
    });

    if (!response.ok) {
        const errText = await response.text();
        throw new Error(`GPT-4o (${response.status}): ${errText}`);
    }

    const result = await response.json();
    const content = result.choices?.[0]?.message?.content || '';
    if (!content) throw new Error('GPT-4o empty response');

    // GPT-4o with json_object mode wraps arrays in an object like { "lessons": [...] }
    let parsed: any;
    try { parsed = JSON.parse(content); } catch {
        throw new Error(`GPT-4o bad JSON: ${content.substring(0, 300)}`);
    }

    // Handle both { lessons: [...] } and direct [...]
    const rawArray = Array.isArray(parsed) ? parsed : (parsed.lessons || parsed.chapters || parsed.data || []);
    if (!Array.isArray(rawArray)) throw new Error('GPT-4o: could not find lessons array');

    const lessons = parseAndSanitize(JSON.stringify(rawArray), pageCount);
    console.log(`[BookSegmenter] GPT-4o found ${lessons.length} lessons`);
    return lessons;
}

// â”€â”€â”€ Multi-Model Orchestrator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * Detects lesson boundaries using multiple AI models with quality validation.
 * 
 * Flow:
 * 1. Try Gemini â†’ validate quality
 * 2. If quality is HIGH (â‰¥0.7) â†’ use Gemini result
 * 3. If quality is MEDIUM (0.5-0.7) â†’ try GPT-4o too, pick best
 * 4. If Gemini FAILS â†’ try GPT-4o alone
 * 5. If both fail â†’ throw (caller will use fallback split)
 */
async function detectLessonBoundaries(
    buffer: Buffer,
    pageCount: number
): Promise<{ lessons: DetectedLesson[]; model: 'gemini' | 'gpt4o' | 'cross-validated' }> {

    let geminiResult: DetectedLesson[] | null = null;
    let geminiQuality: QualityReport | null = null;
    let gpt4oResult: DetectedLesson[] | null = null;
    let gpt4oQuality: QualityReport | null = null;

    // â”€â”€ Step 1: Try Gemini â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try {
        geminiResult = await detectWithGemini(buffer, pageCount);
        geminiQuality = assessQuality(geminiResult, pageCount);
        console.log(`[BookSegmenter] Gemini quality: ${(geminiQuality.score * 100).toFixed(0)}% (${geminiResult.length} lessons, ${(geminiQuality.pageCoverage * 100).toFixed(0)}% coverage)`);
        if (geminiQuality.issues.length > 0) {
            console.log(`[BookSegmenter]   Issues: ${geminiQuality.issues.join(', ')}`);
        }

        // HIGH quality â†’ use directly
        if (geminiQuality.score >= 0.7) {
            console.log(`[BookSegmenter] âœ… Gemini HIGH quality â€” using directly`);
            logLessons(geminiResult, 'Gemini');
            return { lessons: geminiResult, model: 'gemini' };
        }

        console.log(`[BookSegmenter] âš ï¸ Gemini MEDIUM quality â€” cross-checking with GPT-4o...`);
    } catch (geminiErr: any) {
        console.warn(`[BookSegmenter] âŒ Gemini failed: ${geminiErr.message}`);
    }

    // â”€â”€ Step 2: Try GPT-4o â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try {
        gpt4oResult = await detectWithGPT4o(buffer, pageCount);
        gpt4oQuality = assessQuality(gpt4oResult, pageCount);
        console.log(`[BookSegmenter] GPT-4o quality: ${(gpt4oQuality.score * 100).toFixed(0)}% (${gpt4oResult.length} lessons, ${(gpt4oQuality.pageCoverage * 100).toFixed(0)}% coverage)`);
        if (gpt4oQuality.issues.length > 0) {
            console.log(`[BookSegmenter]   Issues: ${gpt4oQuality.issues.join(', ')}`);
        }
    } catch (gptErr: any) {
        console.warn(`[BookSegmenter] âŒ GPT-4o failed: ${gptErr.message}`);
    }

    // â”€â”€ Step 3: Pick best result â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if (geminiResult && gpt4oResult && geminiQuality && gpt4oQuality) {
        // Both succeeded â€” pick higher quality
        if (geminiQuality.score >= gpt4oQuality.score) {
            console.log(`[BookSegmenter] âœ… Cross-validated: Gemini wins (${(geminiQuality.score * 100).toFixed(0)}% vs ${(gpt4oQuality.score * 100).toFixed(0)}%)`);
            logLessons(geminiResult, 'Final (Gemini)');
            return { lessons: geminiResult, model: 'cross-validated' };
        } else {
            console.log(`[BookSegmenter] âœ… Cross-validated: GPT-4o wins (${(gpt4oQuality.score * 100).toFixed(0)}% vs ${(geminiQuality.score * 100).toFixed(0)}%)`);
            logLessons(gpt4oResult, 'Final (GPT-4o)');
            return { lessons: gpt4oResult, model: 'cross-validated' };
        }
    }

    if (geminiResult && geminiQuality && geminiQuality.score >= MIN_QUALITY_SCORE) {
        console.log(`[BookSegmenter] âœ… Using Gemini only (GPT-4o unavailable)`);
        logLessons(geminiResult, 'Gemini');
        return { lessons: geminiResult, model: 'gemini' };
    }

    if (gpt4oResult && gpt4oQuality && gpt4oQuality.score >= MIN_QUALITY_SCORE) {
        console.log(`[BookSegmenter] âœ… Using GPT-4o only (Gemini unavailable)`);
        logLessons(gpt4oResult, 'GPT-4o');
        return { lessons: gpt4oResult, model: 'gpt4o' };
    }

    // Both failed or both below quality threshold
    throw new Error('Both AI models failed or produced low-quality results');
}

function logLessons(lessons: DetectedLesson[], source: string): void {
    for (const l of lessons) {
        console.log(`  ğŸ“– [${source}] "${l.title}" (pages ${l.startPage}-${l.endPage})`);
    }
}

// â”€â”€â”€ Fallback: Equal Page Split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function fallbackSplit(pageCount: number): DetectedLesson[] {
    const lessons: DetectedLesson[] = [];
    const pagesPerLesson = FALLBACK_PAGES_PER_LESSON;
    let page = 1;
    let lessonNum = 1;

    while (page <= pageCount) {
        const endPage = Math.min(page + pagesPerLesson - 1, pageCount);
        lessons.push({
            title: `Ø§Ù„Ø¯Ø±Ø³ ${lessonNum}`,
            startPage: page,
            endPage,
            description: `ØµÙØ­Ø§Øª ${page} Ø¥Ù„Ù‰ ${endPage}`
        });
        page = endPage + 1;
        lessonNum++;
    }

    console.log(`[BookSegmenter] âš ï¸ Fallback: split into ${lessons.length} lessons (${pagesPerLesson} pages each)`);
    return lessons;
}

// â”€â”€â”€ PDF Page Count Helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function getPageCount(buffer: Buffer): Promise<number> {
    try {
        const { PDFParse } = await import('pdf-parse');
        const parser = new PDFParse({ data: new Uint8Array(buffer) });
        const result = await parser.getText();
        const count = result.pages.length;
        await parser.destroy();
        return count || 1;
    } catch {
        // Rough estimate: ~3KB per page for typical Arabic textbooks
        return Math.max(1, Math.round(buffer.byteLength / 3000));
    }
}

// â”€â”€â”€ Per-Lesson PDF Extraction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * Extract text for a specific page range from the PDF using Gemini Vision.
 * This creates chunks in document_sections for the given lessonId.
 */
async function extractLessonPages(
    supabase: SupabaseClient<any, any, any>,
    buffer: Buffer,
    lessonId: string,
    startPage: number,
    endPage: number,
    totalPages: number
): Promise<{ chunksCreated: number; totalChars: number }> {
    const apiKey = process.env.GEMINI_API_KEY;
    if (!apiKey) throw new Error('GEMINI_API_KEY not set');

    const base64 = buffer.toString('base64');
    const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=${apiKey}`;

    console.log(`[BookSegmenter] ğŸ“„ Extracting pages ${startPage}-${endPage} for lesson ${lessonId}...`);

    const response = await fetch(url, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
            contents: [{
                parts: [
                    {
                        text: `Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ ÙÙ‚Ø· ÙÙŠ Ø§Ù„ØµÙØ­Ø§Øª Ù…Ù† ${startPage} Ø¥Ù„Ù‰ ${endPage} Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù PDF (Ø¥Ø¬Ù…Ø§Ù„ÙŠ ${totalPages} ØµÙØ­Ø©).

Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯:
- Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„ØµÙØ­Ø§Øª ${startPage} Ø¥Ù„Ù‰ ${endPage} ÙÙ‚Ø·ØŒ Ù„Ø§ ØªØ³ØªØ®Ø±Ø¬ Ù…Ù† ØµÙØ­Ø§Øª Ø£Ø®Ø±Ù‰
- Ø§ÙƒØªØ¨ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ ÙƒÙ…Ø§ Ù‡Ùˆ Ø¨Ø§Ù„Ø¶Ø¨Ø· Ø¨Ø¯ÙˆÙ† ØªØ¹Ø¯ÙŠÙ„
- Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ ØªØ±ØªÙŠØ¨ Ø§Ù„ÙÙ‚Ø±Ø§Øª ÙˆØ§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
- Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±Ù‚ÙŠÙ… ÙˆØ§Ù„ØªÙ†Ø³ÙŠÙ‚
- Ù„Ø§ ØªØ¶Ù Ø£ÙŠ ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø£Ùˆ Ø´Ø±ÙˆØ­Ø§Øª
- Ù„Ø§ ØªØ®ØªØµØ± â€” Ø§ÙƒØªØ¨ ÙƒÙ„ ÙƒÙ„Ù…Ø©
- Ø£Ø®Ø±Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ ÙÙ‚Ø·`
                    },
                    { inlineData: { data: base64, mimeType: 'application/pdf' } }
                ]
            }],
            generationConfig: { temperature: 0.1, maxOutputTokens: 65536 }
        })
    });

    const data = await response.json();
    if (!response.ok) throw new Error(`Gemini extract: ${data.error?.message || response.status}`);

    const text = data.candidates?.[0]?.content?.parts?.[0]?.text || '';
    if (text.length < 50) {
        console.warn(`[BookSegmenter] âš ï¸ Very short extraction (${text.length} chars) for pages ${startPage}-${endPage}`);
    }

    // Use the existing chunker to chunk and store
    const { chunkText, linkChunks } = await import('./chunker');
    const chunks = chunkText(text);

    if (chunks.length === 0) {
        return { chunksCreated: 0, totalChars: text.length };
    }

    // Clear old sections for this lesson (in case of re-run)
    await supabase.from('document_sections').delete()
        .eq('lesson_id', lessonId).eq('source_type', 'pdf');

    const contentHash = `book-segment-${lessonId}-${startPage}-${endPage}`;
    const sectionsToInsert = chunks.map(chunk => ({
        lesson_id: lessonId,
        content: chunk.content,
        source_type: 'pdf' as const,
        source_file_id: `book-pages-${startPage}-${endPage}`,
        chunk_index: chunk.chunkIndex,
        metadata: {
            content_hash: contentHash,
            start_char: chunk.metadata.startChar,
            end_char: chunk.metadata.endChar,
            token_estimate: chunk.metadata.tokenEstimate,
            extraction_method: 'gemini-vision-segment',
            page_range: { start: startPage, end: endPage }
        }
    }));

    const { data: inserted, error: insertError } = await supabase
        .from('document_sections').insert(sectionsToInsert).select('id');
    if (insertError) throw new Error(`Insert failed: ${insertError.message}`);

    // Link chunks
    if (inserted && inserted.length > 1) {
        const links = linkChunks(inserted.map(r => r.id));
        for (const link of links) {
            if (link.prevId || link.nextId) {
                await supabase.from('document_sections')
                    .update({ prev_id: link.prevId, next_id: link.nextId })
                    .eq('id', link.id);
            }
        }
    }

    console.log(`[BookSegmenter] âœ… Extracted ${inserted?.length || 0} chunks (${text.length} chars) for pages ${startPage}-${endPage}`);
    return { chunksCreated: inserted?.length || 0, totalChars: text.length };
}

// â”€â”€â”€ Main Orchestrator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * Main entry point: Segments a full textbook PDF into individual lessons.
 *
 * Flow:
 * 1. Download PDF from Storage
 * 2. Detect lesson boundaries (AI or fallback)
 * 3. Create lesson records in DB
 * 4. For each lesson: extract pages â†’ embed â†’ analyze
 */
export async function segmentBook(
    supabase: SupabaseClient<any, any, any>,
    subjectId: string,
    userId: string,
    filePath: string,
    options: {
        autoAnalyze?: boolean;    // default: true â€” run full pipeline per lesson
        autoEmbed?: boolean;      // default: true â€” generate embeddings
    } = {}
): Promise<SegmentationResult> {

    const autoAnalyze = options.autoAnalyze !== false;
    const autoEmbed = options.autoEmbed !== false;

    console.log(`[BookSegmenter] ğŸ“š Starting book segmentation`);
    console.log(`[BookSegmenter]    Subject: ${subjectId}`);
    console.log(`[BookSegmenter]    File: ${filePath}`);

    // â•â•â• Step 1: Download PDF â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    const { data: fileData, error: downloadError } = await supabase.storage
        .from('homework-uploads').download(filePath);
    if (downloadError || !fileData) throw new Error(`Download failed: ${downloadError?.message}`);

    const buffer = Buffer.from(await fileData.arrayBuffer());
    console.log(`[BookSegmenter] Downloaded: ${(buffer.byteLength / 1024 / 1024).toFixed(1)} MB`);

    // â•â•â• Step 2: Get page count â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    const pageCount = await getPageCount(buffer);
    console.log(`[BookSegmenter] PDF has ${pageCount} pages`);

    // â•â•â• Step 3: Detect lesson boundaries â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    let detectedLessons: DetectedLesson[];
    let method: SegmentationResult['method'];

    try {
        const detection = await detectLessonBoundaries(buffer, pageCount);
        detectedLessons = detection.lessons;
        if (detectedLessons.length < MIN_LESSONS_DETECTED) {
            throw new Error(`Only ${detectedLessons.length} lessons detected â€” too few`);
        }
        method = detection.model;
    } catch (aiErr: any) {
        console.warn(`[BookSegmenter] âš ï¸ AI detection failed: ${aiErr.message}. Using fallback.`);
        detectedLessons = fallbackSplit(pageCount);
        method = 'fallback-split';
    }

    console.log(`[BookSegmenter] ğŸ“‹ ${detectedLessons.length} lessons detected via ${method}`);

    // â•â•â• Step 4: Create lessons + process each â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    const results: SegmentationResult['lessons'] = [];

    for (let i = 0; i < detectedLessons.length; i++) {
        const detected = detectedLessons[i];
        const lessonId = crypto.randomUUID();

        console.log(`\n[BookSegmenter] â•â•â• Lesson ${i + 1}/${detectedLessons.length}: "${detected.title}" â•â•â•`);

        try {
            // 4a. Create lesson record in DB
            const { error: createError } = await supabase
                .from('lessons')
                .insert({
                    id: lessonId,
                    course_id: subjectId,
                    lesson_title: detected.title,
                    created_by: userId,
                    created_at: new Date().toISOString(),
                    sources: [{
                        id: `segment-${lessonId}`,
                        type: 'pdf',
                        name: `pages ${detected.startPage}-${detected.endPage}`,
                        content: '[auto-segmented]'
                    }],
                    request_type: 'study',
                    analysis_status: 'pending',
                    student_text: JSON.stringify({
                        auto_segmented: true,
                        source_file: filePath,
                        page_range: { start: detected.startPage, end: detected.endPage },
                        description: detected.description || null,
                        segmentation_method: method
                    })
                });

            if (createError) {
                throw new Error(`Create lesson: ${createError.message}`);
            }

            console.log(`[BookSegmenter] âœ… Created lesson: ${lessonId}`);

            // 4b. Extract text for this lesson's page range
            const extraction = await extractLessonPages(
                supabase, buffer, lessonId,
                detected.startPage, detected.endPage, pageCount
            );

            if (extraction.chunksCreated === 0) {
                console.warn(`[BookSegmenter] âš ï¸ No chunks for "${detected.title}" â€” marking as failed`);
                await supabase.from('lessons')
                    .update({ analysis_status: 'failed' })
                    .eq('id', lessonId);
                results.push({
                    id: lessonId, title: detected.title,
                    startPage: detected.startPage, endPage: detected.endPage,
                    status: 'failed', error: 'No content extracted'
                });
                continue;
            }

            let currentStatus: 'created' | 'processed' | 'analyzed' = 'processed';

            // 4c. Generate embeddings (optional)
            if (autoEmbed) {
                try {
                    console.log(`[BookSegmenter] ğŸ”„ Generating embeddings for "${detected.title}"...`);
                    await embedLessonSections(supabase, lessonId);
                } catch (embedErr: any) {
                    console.warn(`[BookSegmenter] âš ï¸ Embeddings failed (non-fatal): ${embedErr.message}`);
                }
            }

            // 4d. Run analysis (optional)
            if (autoAnalyze) {
                try {
                    console.log(`[BookSegmenter] ğŸ§  Analyzing "${detected.title}"...`);
                    await generateLessonAnalysis(supabase, lessonId);
                    currentStatus = 'analyzed';
                } catch (analysisErr: any) {
                    console.warn(`[BookSegmenter] âš ï¸ Analysis failed (non-fatal): ${analysisErr.message}`);
                }
            }

            results.push({
                id: lessonId, title: detected.title,
                startPage: detected.startPage, endPage: detected.endPage,
                status: currentStatus
            });

        } catch (err: any) {
            console.error(`[BookSegmenter] âŒ Lesson "${detected.title}" failed: ${err.message}`);
            results.push({
                id: lessonId, title: detected.title,
                startPage: detected.startPage, endPage: detected.endPage,
                status: 'failed', error: err.message
            });
        }
    }

    // â•â•â• Step 5: Summary â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    const succeeded = results.filter(r => r.status !== 'failed').length;
    const failed = results.filter(r => r.status === 'failed').length;

    console.log(`\n[BookSegmenter] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`);
    console.log(`[BookSegmenter] ğŸ“š Segmentation complete!`);
    console.log(`[BookSegmenter]    Total pages: ${pageCount}`);
    console.log(`[BookSegmenter]    Lessons detected: ${detectedLessons.length}`);
    console.log(`[BookSegmenter]    Succeeded: ${succeeded}`);
    console.log(`[BookSegmenter]    Failed: ${failed}`);
    console.log(`[BookSegmenter]    Method: ${method}`);
    console.log(`[BookSegmenter] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n`);

    return {
        totalPages: pageCount,
        lessonsDetected: detectedLessons.length,
        lessons: results,
        method
    };
}
