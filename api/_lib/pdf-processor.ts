import type { SupabaseClient } from '@supabase/supabase-js';
import { chunkText, linkChunks, TextChunk } from './chunker.js';

/**
 * PDF Text Extraction + Chunking + Storage
 * 
 * Strategy: Gemini Vision ALWAYS PRIMARY for Arabic PDFs.
 * pdf-parse only as last resort (returns garbled Arabic).
 * Retry Gemini once if output is suspiciously short.
 */

// â”€â”€â”€ Gemini Vision (PRIMARY â€” clean Arabic) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function extractWithGemini(buffer: Buffer, pageCount: number, attempt: number = 1): Promise<string> {
    const apiKey = process.env.GEMINI_API_KEY;
    if (!apiKey) throw new Error('GEMINI_API_KEY not set');

    const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=${apiKey}`;
    const INLINE_MAX = 10 * 1024 * 1024; // 10MB inline limit

    console.log(`[PDF] ðŸ”„ Gemini Vision extraction (${pageCount} pages, ${(buffer.byteLength / (1024 * 1024)).toFixed(1)}MB, attempt ${attempt})...`);

    const prompt = attempt === 1
        ? `Ø§Ø³ØªØ®Ø±Ø¬ ÙƒÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù PDF Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙˆØ¨Ø§Ù„ÙƒØ§Ù…Ù„.

Ù‡Ø°Ø§ Ù…Ù„Ù Ø¶Ø®Ù… (${pageCount} ØµÙØ­Ø©). ÙŠØ¬Ø¨ Ø£Ù† ØªØ³ØªØ®Ø±Ø¬ ÙƒÙ„ Ø§Ù„Ù†Øµ Ù…Ù† ÙƒÙ„ ØµÙØ­Ø© Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªØ«Ù†Ø§Ø¡.

âš ï¸ ØªÙ†Ø¨ÙŠÙ‡: Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù Ù‚Ø¯ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØµÙˆØ± Ù…Ù…Ø³ÙˆØ­Ø© (Scanned).

Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯:
- Ø§Ù‚Ø±Ø£ ÙƒÙ„ ØµÙØ­Ø© Ø¨Ù…Ø§ ÙÙŠÙ‡Ø§ Ø§Ù„ØµÙˆØ± â€” Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø¯Ø§Ø®Ù„ Ø§Ù„ØµÙˆØ±
- Ø§ÙƒØªØ¨ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ ÙƒÙ…Ø§ Ù‡Ùˆ Ø¨Ø§Ù„Ø¶Ø¨Ø· Ø¨Ø¯ÙˆÙ† ØªØ¹Ø¯ÙŠÙ„
- Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ ØªØ±ØªÙŠØ¨ Ø§Ù„ÙÙ‚Ø±Ø§Øª ÙˆØ§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ§Ù„Ø£Ù‚Ø³Ø§Ù…
- Ø§Ø³ØªØ®Ø±Ø¬ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©
- Ø§Ø³ØªØ®Ø±Ø¬ Ù…Ù† ÙƒÙ„ Ø§Ù„ØµÙØ­Ø§Øª (Ø§Ù„ØµÙØ­Ø© 1 Ø¥Ù„Ù‰ ${pageCount})
- Ù„Ø§ ØªØ¶Ù Ø£ÙŠ ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø£Ùˆ Ø´Ø±ÙˆØ­Ø§Øª Ù…Ù† Ø¹Ù†Ø¯Ùƒ
- Ù„Ø§ ØªØ®ØªØµØ± â€” Ø§ÙƒØªØ¨ ÙƒÙ„ ÙƒÙ„Ù…Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ù„Ù
- Ø£Ø®Ø±Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ ÙÙ‚Ø·`
        : `Ø£Ø¹Ø¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ù€ PDF Ø¨Ø§Ù„ÙƒØ§Ù…Ù„. Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© ÙƒØ§Ù†Øª Ù†Ø§Ù‚ØµØ©.

âš ï¸ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù ${pageCount} ØµÙØ­Ø© ÙˆØ£ØºÙ„Ø¨Ù‡ ØµÙˆØ± Ù…Ù…Ø³ÙˆØ­Ø©. ÙŠØ¬Ø¨ Ù‚Ø±Ø§Ø¡Ø© ÙƒÙ„ ØµÙØ­Ø© Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªØ«Ù†Ø§Ø¡.

Ø§Ù‚Ø±Ø£ ÙƒÙ„ ØµÙØ­Ø© Ù…Ù† 1 Ø¥Ù„Ù‰ ${pageCount} ÙˆØ§Ø³ØªØ®Ø±Ø¬:
- ÙƒÙ„ Ø§Ù„Ù†ØµÙˆØµ (Ø­ØªÙ‰ Ø§Ù„Ù„ÙŠ Ø¯Ø§Ø®Ù„ Ø§Ù„ØµÙˆØ±)
- Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ§Ù„Ø£Ø´ÙƒØ§Ù„
- Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù…
- Ù„Ø§ ØªØ®ØªØµØ± Ø£Ø¨Ø¯Ø§Ù‹ â€” Ø§ÙƒØªØ¨ ÙƒÙ„ ÙƒÙ„Ù…Ø©`;

    // Always use File API for large files, but do it in ONE call
    let pdfPart: any;
    if (buffer.byteLength > INLINE_MAX) {
        console.log(`[PDF] ðŸ“¤ Large PDF (${(buffer.byteLength / (1024 * 1024)).toFixed(1)}MB), using File API...`);
        const fileUri = await uploadPdfToGemini(buffer, apiKey);
        pdfPart = { fileData: { fileUri, mimeType: 'application/pdf' } };
    } else {
        const base64 = buffer.toString('base64');
        pdfPart = { inlineData: { data: base64, mimeType: 'application/pdf' } };
    }

    const response = await fetch(url, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
            contents: [{ parts: [{ text: prompt }, pdfPart] }],
            generationConfig: { temperature: 0.1, maxOutputTokens: 65536 }
        })
    });

    const data = await response.json();
    if (!response.ok) throw new Error(`Gemini: ${data.error?.message || response.status}`);

    const parts = data.candidates?.[0]?.content?.parts || [];
    const text = parts.filter((p: any) => p.text).map((p: any) => p.text).join('').trim();
    console.log(`[PDF] Gemini Vision attempt ${attempt}: ${text.length} chars`);

    const expectedMinChars = pageCount * 200;
    if (text.length < expectedMinChars && attempt === 1) {
        console.log(`[PDF] âš ï¸ Output too short (${text.length} < expected ${expectedMinChars}). Retrying with stronger prompt...`);
        await new Promise(r => setTimeout(r, 2000));
        return extractWithGemini(buffer, pageCount, 2);
    }

    return text;
}

/** Upload large PDF to Gemini File API (same approach as audio) */
async function uploadPdfToGemini(buffer: Buffer, apiKey: string): Promise<string> {
    // Step 1: Start resumable upload
    const startRes = await fetch(
        `https://generativelanguage.googleapis.com/upload/v1beta/files?key=${apiKey}`,
        {
            method: 'POST',
            headers: {
                'X-Goog-Upload-Protocol': 'resumable',
                'X-Goog-Upload-Command': 'start',
                'X-Goog-Upload-Header-Content-Length': buffer.byteLength.toString(),
                'X-Goog-Upload-Header-Content-Type': 'application/pdf',
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({ file: { displayName: 'document.pdf' } })
        }
    );
    if (!startRes.ok) throw new Error(`File API start: ${startRes.status} ${await startRes.text()}`);
    const uploadUrl = startRes.headers.get('X-Goog-Upload-URL');
    if (!uploadUrl) throw new Error('No upload URL');

    // Step 2: Upload file
    const uploadRes = await fetch(uploadUrl, {
        method: 'PUT',
        headers: {
            'Content-Length': buffer.byteLength.toString(),
            'X-Goog-Upload-Offset': '0',
            'X-Goog-Upload-Command': 'upload, finalize'
        },
        body: new Uint8Array(buffer)
    });
    if (!uploadRes.ok) throw new Error(`File API upload: ${uploadRes.status}`);
    const fileInfo = await uploadRes.json();
    const fileUri = fileInfo.file?.uri;
    if (!fileUri) throw new Error('No file URI');
    console.log(`[PDF] âœ… Uploaded to File API: ${fileUri}`);

    // Step 3: Wait for ACTIVE
    const fileName = fileInfo.file?.name;
    for (let i = 0; i < 30; i++) {
        const res = await fetch(`https://generativelanguage.googleapis.com/v1beta/${fileName}?key=${apiKey}`);
        const status = await res.json();
        if (status.state === 'ACTIVE') return fileUri;
        if (status.state === 'FAILED') throw new Error('PDF processing failed');
        console.log(`[PDF] â³ File state: ${status.state}...`);
        await new Promise(r => setTimeout(r, 2000));
    }
    throw new Error('PDF processing timeout');
}

// â”€â”€â”€ pdf-parse (FALLBACK â€” all pages but garbled Arabic) â”€

async function extractWithPdfParse(buffer: Buffer): Promise<{ text: string; pages: number }> {
    const { PDFParse } = await import('pdf-parse');
    const parser = new PDFParse({ data: new Uint8Array(buffer) });
    const textResult = await parser.getText();
    await parser.destroy();

    let text = textResult.text.normalize('NFKC');
    text = text
        .replace(/[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]/g, '')
        .replace(/\r\n/g, '\n')
        .replace(/[ \t]+/g, ' ')
        .replace(/\n{3,}/g, '\n\n')
        .trim();

    return { text, pages: textResult.pages.length };
}

// â”€â”€â”€ Main Pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export async function processPdfJob(
    supabase: SupabaseClient<any, any, any>,
    lessonId: string,
    filePath: string,
    contentHash: string
): Promise<{ chunksCreated: number; totalChars: number; method: string }> {

    let fileData: Blob | null = null;
    let downloadError: any = null;

    // Retry download up to 3 times for large files
    for (let attempt = 1; attempt <= 3; attempt++) {
        console.log(`[PDF] ðŸ“¥ Downloading from Supabase (attempt ${attempt}/3)...`);
        const { data, error } = await supabase.storage.from('homework-uploads').download(filePath);
        if (!error && data) {
            fileData = data;
            break;
        }
        downloadError = error;
        console.warn(`[PDF] âš ï¸ Download attempt ${attempt} failed: ${error?.message || 'Network error'}`);
        if (attempt < 3) await new Promise(r => setTimeout(r, 2000));
    }

    if (!fileData) throw new Error(`Download failed after 3 attempts: ${downloadError?.message || 'Unknown error'}`);

    const buffer = Buffer.from(await fileData.arrayBuffer());
    console.log(`[PDF] âœ… Downloaded: ${(buffer.byteLength / 1024).toFixed(1)} KB`);

    // Get page count from pdf-parse (fast, always works)
    let pdfParseText = '';
    let pageCount = 0;

    try {
        const result = await extractWithPdfParse(buffer);
        pdfParseText = result.text;
        pageCount = result.pages;
        console.log(`[PDF] pdf-parse: ${pageCount} pages, ${pdfParseText.length} chars`);
    } catch (e: any) {
        console.warn(`[PDF] pdf-parse failed: ${e.message}`);
    }

    // Always try Gemini Vision first (best Arabic quality)
    let finalText = '';
    let method = 'none';

    try {
        const geminiText = await extractWithGemini(buffer, pageCount || 1);

        if (geminiText.length >= 200) {
            finalText = geminiText;
            method = 'gemini-vision';
            console.log(`[PDF] âœ… Using Gemini Vision: ${finalText.length} chars`);
        } else {
            console.warn(`[PDF] Gemini returned too little: ${geminiText.length} chars`);
        }
    } catch (e: any) {
        console.warn(`[PDF] Gemini Vision failed: ${e.message}`);
    }

    // Fallback to pdf-parse only if Gemini completely failed
    if (!finalText && pdfParseText.length >= 200) {
        finalText = pdfParseText;
        method = 'pdf-parse';
        console.log(`[PDF] âš ï¸ Falling back to pdf-parse: ${finalText.length} chars (may be garbled)`);
    }

    if (!finalText) throw new Error('PDF extraction failed: no usable text');

    console.log(`[PDF] Final: ${finalText.length} chars via ${method}`);
    console.log(`[PDF] Preview: "${finalText.substring(0, 200).replace(/\n/g, ' ')}..."`);

    // â”€â”€â”€ Chunk + Store â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const chunks: TextChunk[] = chunkText(finalText);
    console.log(`[PDF] Chunked: ${chunks.length} chunks`);
    if (chunks.length === 0) throw new Error('No chunks created');

    await supabase.from('document_sections').delete()
        .eq('lesson_id', lessonId)
        .eq('source_type', 'pdf')
        .eq('source_file_id', filePath);

    const sectionsToInsert = chunks.map(chunk => ({
        lesson_id: lessonId,
        content: chunk.content,
        source_type: 'pdf' as const,
        source_file_id: filePath,
        chunk_index: chunk.chunkIndex,
        metadata: {
            content_hash: contentHash,
            start_char: chunk.metadata.startChar,
            end_char: chunk.metadata.endChar,
            token_estimate: chunk.metadata.tokenEstimate,
            extraction_method: method
        }
    }));

    const { data: inserted, error: insertError } = await supabase
        .from('document_sections').insert(sectionsToInsert).select('id');
    if (insertError) throw new Error(`Insert failed: ${insertError.message}`);

    if (inserted && inserted.length > 1) {
        const links = linkChunks(inserted.map(r => r.id));
        for (const link of links) {
            if (link.prevId || link.nextId) {
                await supabase.from('document_sections')
                    .update({ prev_id: link.prevId, next_id: link.nextId })
                    .eq('id', link.id);
            }
        }
    }

    console.log(`[PDF] âœ… Done: ${inserted?.length || 0} chunks saved`);
    return { chunksCreated: inserted?.length || 0, totalChars: finalText.length, method };
}
